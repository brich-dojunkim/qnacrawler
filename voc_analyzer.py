# voc_analyzer.py (import ê²½ë¡œ ìˆ˜ì •)
import json
import pandas as pd
import numpy as np
from collections import Counter
from datetime import datetime
from typing import Dict, List, Tuple
import warnings
warnings.filterwarnings('ignore')

from output_manager import setup_output_dirs, get_analysis_filename

class CategoryBasedVoCAnalyzer:
    def __init__(self, json_file_path: str):
        """
        ì¹´í…Œê³ ë¦¬ ê¸°ë°˜ VoC ë¶„ì„ê¸° ì´ˆê¸°í™”
        
        Args:
            json_file_path: í¬ë¡¤ë§ëœ JSON íŒŒì¼ ê²½ë¡œ
        """
        self.json_file_path = json_file_path
        self.df = None
        
        # ìœ ì € ì—¬ì •ë³„ ì¹´í…Œê³ ë¦¬ ë§¤í•‘
        self.user_journey_mapping = {
            'ê³„ì •Â·ì…ì ': [
                'ì…ì ê´€ë¦¬', 'ìŠ¤í† ì–´ê´€ë¦¬', 'í”Œëœê´€ë¦¬', 'ì‹ ê·œíšŒì›ê°€ì…',
                'ì‚¬ì—…ìì •ë³´/ì–‘ë„ì–‘ìˆ˜', 'íƒˆí‡´/ì¬ê°€ì…', 'ë¸Œëœë“œê¶Œí•œì‹ ì²­'
            ],
            'ìƒí’ˆÂ·ì½˜í…ì¸ ': [
                'ìƒí’ˆë“±ë¡', 'ìƒí’ˆë“±ë¡ ì‹¤íŒ¨', 'ìƒí’ˆ ì¡°íšŒ ë° ìˆ˜ì •', 'ì±„ë„ìƒí’ˆì—°ë™',
                'ë¸Œë¦¬ì¹˜ ê¸°íšì „ì‹ ì²­', 'ì±„ë„ë”œ ì§„í–‰ê´€ë¦¬', 'ìƒí’ˆë¬¸ì˜(ë¸Œë¦¬ì¹˜)', 'ìƒí’ˆë¬¸ì˜(ì±„ë„)'
            ],
            'ì£¼ë¬¸Â·ë°°ì†¡': [
                'ë°œì£¼/ë°œì†¡ê´€ë¦¬', 'ë°°ì†¡í˜„í™©ê´€ë¦¬', 'ë°°ì†¡ì§€ì—° ê´€ë¦¬ (ê²°í’ˆì·¨ì†Œ)',
                'ì†¡ì¥ë“±ë¡ ì‹¤íŒ¨/ ì†¡ì¥ë²ˆí˜¸ ìˆ˜ì •', 'ì£¼ë¬¸ì¡°íšŒ', 'ê¸´ê¸‰ë¬¸ì˜', 'ë°°ì†¡ì •ì±… ê´€ë¦¬'
            ],
            'ë°˜í’ˆÂ·ì·¨ì†Œ': [
                'ì·¨ì†Œê´€ë¦¬', 'êµí™˜ê´€ë¦¬/êµí™˜ì² íšŒ', 'ë°˜í’ˆê´€ë¦¬/í™˜ë¶ˆë³´ë¥˜'
            ],
            'ì •ì‚°': [
                'êµ¬ë§¤í™•ì •ê´€ë¦¬', 'ì •ì‚°í†µí•©', 'íŠ¹ì•½ë§¤ì…ì •ì‚°', 'íŒë§¤ëŒ€í–‰ì •ì‚°'
            ]
        }
        
        self.load_and_preprocess_data()
        
    def load_and_preprocess_data(self):
        """ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬"""
        try:
            with open(self.json_file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # ë°ì´í„° êµ¬ì¡° í™•ì¸
            if 'data' in data:
                qna_data = data['data']
            else:
                qna_data = data
            
            self.df = pd.DataFrame(qna_data)
            
            # ì¹´í…Œê³ ë¦¬ ì •ë³´ í‰íƒ„í™”
            if 'category' in self.df.columns:
                category_df = pd.json_normalize(self.df['category'])
                self.df = pd.concat([self.df.drop('category', axis=1), category_df], axis=1)
            
            # ë‚ ì§œ ë³€í™˜
            if 'registration_date' in self.df.columns:
                self.df['registration_date'] = pd.to_datetime(self.df['registration_date'], errors='coerce')
            
            # í…ìŠ¤íŠ¸ ê¸¸ì´ ê³„ì‚°
            if 'question_content' in self.df.columns:
                self.df['content_length'] = self.df['question_content'].str.len()
            
            print(f"âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(self.df)}ê°œ ë¬¸ì˜")
            print(f"ğŸ“Š ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ëŸ¼: {list(self.df.columns)}")
            
        except Exception as e:
            print(f"âŒ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
            raise

    def analyze_by_assigned_team(self) -> Dict:
        """assigned_team ê¸°ì¤€ ë¶„ì„"""
        print("ğŸ¢ íŒ€ë³„ ì‹¤ì œ ë¬¸ì˜ ë‚´ìš© ë¶„ì„ ì¤‘...")
        
        if 'assigned_team' not in self.df.columns or 'question_content' not in self.df.columns:
            return {"error": "í•„ìš”í•œ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤."}
        
        team_analysis = {}
        
        for team in self.df['assigned_team'].dropna().unique():
            team_data = self.df[self.df['assigned_team'] == team]
            
            if len(team_data) < 1:
                continue
            
            # ê¸°ë³¸ ì •ë³´
            basic_info = {
                'total_inquiries': len(team_data),
                'urgent_count': team_data['is_urgent'].sum() if 'is_urgent' in team_data.columns else 0,
                'answered_count': len(team_data[team_data['answer_status'] == 'ë‹µë³€ì™„ë£Œ']) if 'answer_status' in team_data.columns else 0,
                'avg_content_length': round(team_data['content_length'].mean(), 1) if 'content_length' in team_data.columns else 0
            }
            
            # ëŒ€í‘œ ë¬¸ì˜ ì‚¬ë¡€ë“¤
            samples = []
            if 'content_length' in team_data.columns:
                sorted_data = team_data.sort_values('content_length')
                
                for quantile in [0.3, 0.7]:
                    idx = int(len(sorted_data) * quantile)
                    if idx < len(sorted_data):
                        sample = sorted_data.iloc[idx]
                        samples.append({
                            'inquiry_id': sample.get('inquiry_id', 'N/A'),
                            'content': sample['question_content'],
                            'length': sample['content_length'],
                            'sub_category': sample.get('sub_category', 'N/A'),
                            'is_urgent': sample.get('is_urgent', False)
                        })
            
            # ì„¸ë¶€ ì¹´í…Œê³ ë¦¬ ë¶„í¬
            sub_categories = {}
            if 'sub_category' in team_data.columns:
                sub_cat_counts = team_data['sub_category'].value_counts().head(5)
                sub_categories = sub_cat_counts.to_dict()
            
            team_analysis[team] = {
                'basic_info': basic_info,
                'sample_inquiries': samples,
                'sub_categories': sub_categories
            }
        
        return team_analysis

    def analyze_by_sub_category(self) -> Dict:
        """sub_category ê¸°ì¤€ ë¶„ì„"""
        print("ğŸ“‚ ì„¸ë¶€ ì¹´í…Œê³ ë¦¬ë³„ ì‹¤ì œ ë¬¸ì˜ ë‚´ìš© ë¶„ì„ ì¤‘...")
        
        if 'sub_category' not in self.df.columns or 'question_content' not in self.df.columns:
            return {"error": "í•„ìš”í•œ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤."}
        
        category_analysis = {}
        
        for category in self.df['sub_category'].dropna().unique():
            cat_data = self.df[self.df['sub_category'] == category]
            
            # ê¸°ë³¸ ì •ë³´
            basic_info = {
                'total_inquiries': len(cat_data),
                'urgent_count': cat_data['is_urgent'].sum() if 'is_urgent' in cat_data.columns else 0,
                'avg_content_length': round(cat_data['content_length'].mean(), 1) if 'content_length' in cat_data.columns else 0
            }
            
            # ì´ ì¹´í…Œê³ ë¦¬ê°€ ì£¼ë¡œ ì–´ëŠ íŒ€ì—ì„œ ì²˜ë¦¬ë˜ëŠ”ì§€
            team_distribution = {}
            if 'assigned_team' in cat_data.columns:
                team_counts = cat_data['assigned_team'].value_counts()
                team_distribution = team_counts.to_dict()
            
            # ëŒ€í‘œ ë¬¸ì˜ ì‚¬ë¡€ë“¤
            samples = []
            sample_count = min(2, len(cat_data))
            for i in range(sample_count):
                sample = cat_data.iloc[i]
                samples.append({
                    'inquiry_id': sample.get('inquiry_id', 'N/A'),
                    'content': sample['question_content'][:150] + '...' if len(sample['question_content']) > 150 else sample['question_content'],
                    'assigned_team': sample.get('assigned_team', 'N/A'),
                    'length': sample.get('content_length', 0),
                    'is_urgent': sample.get('is_urgent', False)
                })
            
            category_analysis[category] = {
                'basic_info': basic_info,
                'team_distribution': team_distribution,
                'sample_inquiries': samples
            }
        
        return category_analysis

    def analyze_by_user_journey(self) -> Dict:
        """user_journey ê¸°ì¤€ ë¶„ì„"""
        print("ğŸ¯ ìœ ì € ì—¬ì •ë³„ ì‹¤ì œ ë¬¸ì˜ ë‚´ìš© ë¶„ì„ ì¤‘...")
        
        if 'sub_category' not in self.df.columns or 'question_content' not in self.df.columns:
            return {"error": "í•„ìš”í•œ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤."}
        
        # ìœ ì € ì—¬ì •ë³„ë¡œ sub_category ë§¤í•‘
        self.df['user_journey'] = self.df['sub_category'].apply(self._map_to_user_journey)
        
        journey_analysis = {}
        
        for journey in self.user_journey_mapping.keys():
            journey_data = self.df[self.df['user_journey'] == journey]
            
            if len(journey_data) < 1:
                continue
            
            # ê¸°ë³¸ ì •ë³´
            basic_info = {
                'total_inquiries': len(journey_data),
                'urgent_count': journey_data['is_urgent'].sum() if 'is_urgent' in journey_data.columns else 0,
                'answered_count': len(journey_data[journey_data['answer_status'] == 'ë‹µë³€ì™„ë£Œ']) if 'answer_status' in journey_data.columns else 0,
                'avg_content_length': round(journey_data['content_length'].mean(), 1) if 'content_length' in journey_data.columns else 0
            }
            
            # ëŒ€í‘œ ë¬¸ì˜ ì‚¬ë¡€ë“¤
            samples = []
            if 'content_length' in journey_data.columns:
                sorted_data = journey_data.sort_values('content_length')
                
                for quantile in [0.3, 0.7]:
                    idx = int(len(sorted_data) * quantile)
                    if idx < len(sorted_data):
                        sample = sorted_data.iloc[idx]
                        samples.append({
                            'inquiry_id': sample.get('inquiry_id', 'N/A'),
                            'content': sample['question_content'],
                            'length': sample['content_length'],
                            'sub_category': sample.get('sub_category', 'N/A'),
                            'assigned_team': sample.get('assigned_team', 'N/A'),
                            'is_urgent': sample.get('is_urgent', False)
                        })
            
            # ì„¸ë¶€ ì¹´í…Œê³ ë¦¬ ë¶„í¬
            sub_categories = {}
            if 'sub_category' in journey_data.columns:
                sub_cat_counts = journey_data['sub_category'].value_counts().head(5)
                sub_categories = sub_cat_counts.to_dict()
            
            # ë‹´ë‹¹íŒ€ ë¶„í¬
            team_distribution = {}
            if 'assigned_team' in journey_data.columns:
                team_counts = journey_data['assigned_team'].value_counts()
                team_distribution = team_counts.to_dict()
            
            journey_analysis[journey] = {
                'basic_info': basic_info,
                'sample_inquiries': samples,
                'sub_categories': sub_categories,
                'team_distribution': team_distribution
            }
        
        return journey_analysis
    
    def _map_to_user_journey(self, sub_category):
        """ì„¸ë¶€ ì¹´í…Œê³ ë¦¬ë¥¼ ìœ ì € ì—¬ì •ìœ¼ë¡œ ë§¤í•‘"""
        if pd.isna(sub_category):
            return 'ê¸°íƒ€'
        
        for journey, categories in self.user_journey_mapping.items():
            if sub_category in categories:
                return journey
        
        return 'ê¸°íƒ€'

    def analyze_weekly_trends(self) -> Dict:
        """ì£¼ê°„ë³„ ë¬¸ì˜ íŠ¸ë Œë“œ"""
        print("ğŸ“… ì£¼ê°„ë³„ ë¬¸ì˜ íŠ¸ë Œë“œ ë¶„ì„ ì¤‘...")
        
        if 'registration_date' not in self.df.columns:
            return {"error": "registration_date ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤."}
        
        # ì£¼ê°„ë³„ ì§‘ê³„
        self.df['year_week'] = self.df['registration_date'].dt.to_period('W-MON')
        weekly_stats = {}
        
        # ìµœê·¼ 12ì£¼ê°„ë§Œ ë¶„ì„
        recent_weeks = self.df['year_week'].dropna().unique()
        recent_weeks = sorted(recent_weeks)[-12:]
        
        for week in recent_weeks:
            week_data = self.df[self.df['year_week'] == week]
            
            stats = {
                'total_inquiries': len(week_data),
                'urgent_count': week_data['is_urgent'].sum() if 'is_urgent' in week_data.columns else 0,
                'avg_content_length': round(week_data['content_length'].mean(), 1) if 'content_length' in week_data.columns else 0
            }
            
            # íŒ€ë³„ ë¶„í¬ (ìƒìœ„ 3ê°œë§Œ)
            if 'assigned_team' in week_data.columns:
                team_dist = week_data['assigned_team'].value_counts().head(3).to_dict()
                stats['top_teams'] = team_dist
            
            weekly_stats[str(week)] = stats
        
        return weekly_stats

    def get_overall_summary(self) -> Dict:
        """ì „ì²´ ë°ì´í„° ìš”ì•½"""
        summary = {
            'total_inquiries': len(self.df),
            'date_range': {
                'start': str(self.df['registration_date'].min().date()) if 'registration_date' in self.df.columns else None,
                'end': str(self.df['registration_date'].max().date()) if 'registration_date' in self.df.columns else None
            }
        }
        
        # íŒ€ ê°œìˆ˜ ë° ëª©ë¡
        if 'assigned_team' in self.df.columns:
            teams = self.df['assigned_team'].dropna().unique()
            summary['teams'] = {
                'count': len(teams),
                'list': teams.tolist()
            }
        
        # ì¹´í…Œê³ ë¦¬ ê°œìˆ˜ ë° ëª©ë¡
        if 'sub_category' in self.df.columns:
            categories = self.df['sub_category'].dropna().unique()
            summary['categories'] = {
                'count': len(categories),
                'list': categories.tolist()
            }
        
        # ê¸°ë³¸ í†µê³„
        if 'is_urgent' in self.df.columns:
            summary['urgent_count'] = int(self.df['is_urgent'].sum())
        
        if 'answer_status' in self.df.columns:
            answer_stats = self.df['answer_status'].value_counts().to_dict()
            summary['answer_status_distribution'] = answer_stats
        
        if 'content_length' in self.df.columns:
            summary['content_length_stats'] = {
                'mean': round(self.df['content_length'].mean(), 1),
                'median': round(self.df['content_length'].median(), 1),
                'min': int(self.df['content_length'].min()),
                'max': int(self.df['content_length'].max())
            }
        
        return summary

    def generate_category_voc_analysis(self, verbose=False) -> Dict:
        """ì¹´í…Œê³ ë¦¬ ê¸°ë°˜ VoC ë¶„ì„ ì‹¤í–‰"""
        if verbose:
            print("ğŸ¯ ì¹´í…Œê³ ë¦¬ ê¸°ë°˜ VoC ë¶„ì„ ì‹œì‘...")
        
        results = {
            "analysis_timestamp": datetime.now().isoformat(),
            "data_source": self.json_file_path,
            "overall_summary": self.get_overall_summary()
        }
        
        # ë¶„ì„ ì‹¤í–‰
        if verbose:
            print("1ï¸âƒ£ íŒ€ë³„ ë¶„ì„...")
        results["team_analysis"] = self.analyze_by_assigned_team()
        
        if verbose:
            print("2ï¸âƒ£ ì„¸ë¶€ ì¹´í…Œê³ ë¦¬ë³„ ë¶„ì„...")
        results["category_analysis"] = self.analyze_by_sub_category()
        
        if verbose:
            print("3ï¸âƒ£ ìœ ì € ì—¬ì •ë³„ ë¶„ì„...")
        results["journey_analysis"] = self.analyze_by_user_journey()
        
        if verbose:
            print("4ï¸âƒ£ ì£¼ê°„ë³„ íŠ¸ë Œë“œ...")
        results["weekly_trends"] = self.analyze_weekly_trends()
        
        # ê²°ê³¼ ì €ì¥ - output í´ë”ì—!
        if verbose:
            output_file = get_analysis_filename()
            
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2, default=str)
            
            print(f"âœ… ë¶„ì„ ê²°ê³¼ ì €ì¥: {output_file}")
        
        return results

# ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
def main():
    import os
    
    # output í´ë” ì„¤ì •
    setup_output_dirs()
    
    # JSON íŒŒì¼ ì°¾ê¸° - í˜„ì¬ í´ë”ì™€ output/crawl_data/ ë‘˜ ë‹¤ í™•ì¸
    json_files = []
    
    # í˜„ì¬ í´ë”ì—ì„œ ì°¾ê¸° (ê¸°ì¡´ íŒŒì¼ë“¤)
    current_files = [f for f in os.listdir('.') if f.endswith('.json') and 'qna' in f.lower()]
    json_files.extend(current_files)
    
    # output/crawl_data/ í´ë”ì—ì„œ ì°¾ê¸° (ìƒˆë¡œ ìƒì„±ëœ íŒŒì¼ë“¤)
    crawl_dir = "output/crawl_data"
    if os.path.exists(crawl_dir):
        crawl_files = [os.path.join(crawl_dir, f) for f in os.listdir(crawl_dir) if f.endswith('.json')]
        json_files.extend(crawl_files)
    
    if not json_files:
        print("âŒ Q&A JSON íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # ê°€ì¥ ìµœê·¼ íŒŒì¼ ì„ íƒ
    json_file = max(json_files, key=os.path.getctime)
    print(f"ğŸ“ ë¶„ì„í•  íŒŒì¼: {json_file}")
    
    try:
        # ë¶„ì„ê¸° ì´ˆê¸°í™”
        analyzer = CategoryBasedVoCAnalyzer(json_file)
        
        # ë¶„ì„ ì‹¤í–‰
        results = analyzer.generate_category_voc_analysis(verbose=True)
        
        # HTML ë³´ê³ ì„œ ìƒì„± (ìˆ˜ì •ëœ import)
        try:
            from voc_html_reporter import CategoryVoCHTMLReporter
            
            html_reporter = CategoryVoCHTMLReporter(analyzer.df)
            html_filename = html_reporter.save_and_open_html_report(results)
            
            print(f"\nğŸ‰ 1ë‹¨ê³„ í†µí•© ë¶„ì„ ì™„ë£Œ!")
            print(f"ğŸ“Š ë¶„ì„ ê²°ê³¼: output/analysis/")
            print(f"ğŸ“„ HTML ë³´ê³ ì„œ: output/reports/")
            
        except ImportError as e:
            print(f"HTML ë¦¬í¬í„° import ì˜¤ë¥˜: {e}")
            print("voc_html_reporter.py íŒŒì¼ì´ ìˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
        
    except Exception as e:
        print(f"âŒ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")

if __name__ == "__main__":
    main()